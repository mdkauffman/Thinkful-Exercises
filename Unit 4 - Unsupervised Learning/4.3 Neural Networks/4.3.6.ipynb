{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Create a multi-layer perceptron neural network model to predict on a labeled dataset of your choosing. Compare this model to either a boosted tree or a random forest model and describe the relative tradeoffs between complexity and accuracy. Be sure to vary the hyperparameters of your MLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "I managed to achieve near parity between a half-optimized MLP (65%) and a completely unoptimized RFC model (64%).  Previously, I had optimized a GBC algorithm to the point of ~71% performance, but that involved running multiple GridSearchCV optimizations and significant tinkering, neither of which I had time for on this project, given the run-time for an MLP.\n",
    "\n",
    "Other caveats for this exercise are:\n",
    "- the previous 71% was achieved with a sample size five times as large; I cut the sample size down to make the run-times for the MLP algorithm more manageable, but this understandably harms the algorithms' ability to learn accurately.\n",
    "- The MLP has not been optimized to the extent that its performance is as high as it can go, only to the point where I am no longer able to make consistent improvement in its performance via tinkering with parameters and layer settings.\n",
    "- This is an incredibly complex dataset, with multiple classes sharing many properties and broad swathes of outliers in every class that seem to defy classification; testing any algorithm on it is going to be challenging.\n",
    "\n",
    "\n",
    "### Analysis\n",
    "While the MLP scores slightly higher than the RFC, it's also notably more over-fitted, so there's a definite trade-off there.  Complexity-wise, MLP is _significantly_ slower to run, and requires more work to optimize than RFC, or even GBC, for (to my eyes) less resulting performance.  Perhaps if I'd had a week to tweak the parameters, run GridSearchCV, and/or use the full dataset, MLP might be able to do as well as the decision-tree models, but for this particular case it's distinctly sub-par from a practical standpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC results:\n",
      " \n",
      "Model score:\n",
      "0.6442185514612452\n",
      " \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.55      0.53       529\n",
      "           1       0.31      0.22      0.26        63\n",
      "           2       0.68      0.72      0.70        29\n",
      "           3       0.46      0.33      0.38       122\n",
      "           4       0.76      0.55      0.64        40\n",
      "           5       0.74      0.79      0.76       855\n",
      "           6       0.55      0.36      0.44        44\n",
      "           7       0.73      0.76      0.75       207\n",
      "           8       0.71      0.45      0.56        11\n",
      "           9       0.49      0.49      0.49       432\n",
      "          10       0.53      0.49      0.51        63\n",
      "          11       0.60      0.47      0.53       131\n",
      "          12       0.83      0.86      0.85       424\n",
      "          13       0.62      0.60      0.61       198\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      3148\n",
      "   macro avg       0.61      0.55      0.57      3148\n",
      "weighted avg       0.64      0.64      0.64      3148\n",
      "\n",
      " \n",
      "Model cross-valuation:\n",
      "[0.63937008 0.65402844 0.62539683 0.64593301 0.66934189]\n",
      "\u0007\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score:\n",
      "0.6550190597204575\n",
      " \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.52      0.53       529\n",
      "           1       0.28      0.24      0.26        63\n",
      "           2       0.70      0.55      0.62        29\n",
      "           3       0.44      0.25      0.32       122\n",
      "           4       0.70      0.57      0.63        40\n",
      "           5       0.76      0.81      0.78       855\n",
      "           6       0.38      0.52      0.44        44\n",
      "           7       0.74      0.71      0.73       207\n",
      "           8       0.70      0.64      0.67        11\n",
      "           9       0.52      0.55      0.53       432\n",
      "          10       0.59      0.57      0.58        63\n",
      "          11       0.57      0.50      0.53       131\n",
      "          12       0.84      0.87      0.86       424\n",
      "          13       0.57      0.64      0.60       198\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      3148\n",
      "   macro avg       0.60      0.57      0.58      3148\n",
      "weighted avg       0.65      0.66      0.65      3148\n",
      "\n",
      " \n",
      "Model cross-valuation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61417323 0.60663507 0.59206349 0.63476874 0.63402889]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(200,150,100,50,25),\n",
    "                    activation= 'logistic', \n",
    "                    solver='adam', \n",
    "                    alpha=0.0001, \n",
    "                    batch_size='auto', \n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.001, \n",
    "                    power_t=0.5, \n",
    "                    max_iter=500, \n",
    "                    shuffle=True, \n",
    "                    random_state=105, \n",
    "                    tol=0.0001, \n",
    "                    verbose=False, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, \n",
    "                    nesterovs_momentum=True, \n",
    "                    early_stopping=False, \n",
    "                    validation_fraction=0.1, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08, \n",
    "                    n_iter_no_change=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "import math\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import os\n",
    "\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload dataset\n",
    "basedata_beer_recipes = pd.read_csv('recipeData.csv', index_col='BeerID', encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am creating functions to allow for repeated processing of different data sets (or different samples of the same data set) without using up a massive number of lines or running the risk of the data cleaning sections of the notebook being different due to a forgetful programmer going in and changing things in one cell but not another.  With a standardized set of functions, they can all be put through the *same* processing, without worrying as much about human error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop excess columns\n",
    "def drop_unnecessary_columns(beer_recipes):\n",
    "    # Drop columns that are useless for our purposes\n",
    "    beer_recipes.drop(['Name', 'URL', 'StyleID', 'UserId'], axis=1, inplace=True)\n",
    "\n",
    "    # Drop columns that have a high % of NaN\n",
    "    beer_recipes.drop(['PrimingMethod', 'PrimingAmount'], axis=1, inplace=True)\n",
    "\n",
    "    return(beer_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column that takes the style of beer and converts it into one of a shorter list of broad styles\n",
    "# Note that the order of these has been changed since v1 of the notebook\n",
    "def add_broad_styles(beer_recipes):\n",
    "    broad_styles = ['English Brown Ale', 'IPA', 'Pale Ale', 'Lager', 'Stout', \n",
    "                    'Bitter', 'Cider', 'Porter', 'Mead', 'Pilsner', \n",
    "                    'Weizen', 'Saison', 'Kölsch', 'American Wheat', \n",
    "                    'Fruit Beer','Barleywine', 'Ale']\n",
    "    beer_recipes['BroadStyle'] = 'Other'\n",
    "    beer_recipes['Style'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    for broad_style in broad_styles:\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains(broad_style), 'BroadStyle'] = broad_style\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Pils'), 'BroadStyle'] = 'Pilsner'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Melomel'), 'BroadStyle'] = 'Mead'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Metheglin'), 'BroadStyle'] = 'Mead'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Witbier'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Weiss'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('weiz'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Gose'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Schwarzbier'), 'BroadStyle'] = 'Lager'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('bock'), 'BroadStyle'] = 'Bock'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Winter'), 'BroadStyle'] = 'Winter Beer'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Tripel'), 'BroadStyle'] = 'Pale Ale'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Bock'), 'BroadStyle'] = 'Bock'\n",
    "    \n",
    "    drop_Other_style = beer_recipes.loc[beer_recipes['BroadStyle'] == 'Other']\n",
    "    beer_recipes.drop(drop_Other_style.index, inplace=True)\n",
    "    \n",
    "    # drop \"Style\" so that the models don't get a chance to cheat\n",
    "    beer_recipes.drop(['Style'], axis=1, inplace=True)\n",
    "\n",
    "    return(beer_recipes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sg_from_plato(plato):\n",
    "    sg = 1 + (plato / (258.6 - ( (plato/258.2) *227.1) ) )\n",
    "    return sg\n",
    "\n",
    "def apply_get_sg(beer_recipes):\n",
    "    beer_recipes['OG_sg'] = beer_recipes.apply(lambda row: \n",
    "                                             get_sg_from_plato(row['OG']) \n",
    "                                             if row['SugarScale'] == 'Plato' \n",
    "                                             else row['OG'], axis=1)\n",
    "    \n",
    "    beer_recipes['FG_sg'] = beer_recipes.apply(lambda row: \n",
    "                                             get_sg_from_plato(row['FG']) \n",
    "                                             if row['SugarScale'] == 'Plato' \n",
    "                                             else row['FG'], axis=1)\n",
    "    \n",
    "    beer_recipes['BoilGravity_sg'] = beer_recipes.apply(lambda row: \n",
    "                                                      get_sg_from_plato(row['BoilGravity']) \n",
    "                                                      if row['SugarScale'] == 'Plato' \n",
    "                                                      else row['BoilGravity'], axis=1)\n",
    "\n",
    "    # Drop the original rows, now that the issue's been dealt with\n",
    "    beer_recipes.drop(['SugarScale','OG','FG','BoilGravity'], axis=1, inplace=True)\n",
    "    \n",
    "    return(beer_recipes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a function similar to the one below, outliers were identified and functions designed to remove them.\n",
    "#beer_recipe.loc[beer_recipe['Size(L)'] > 2300].count()\n",
    "\n",
    "def outlier_cleaner(beer_recipes, feature, outlier_limit):\n",
    "    drop_feature_outliers = beer_recipes.loc[beer_recipes[feature] > outlier_limit]\n",
    "    beer_recipes.drop(drop_feature_outliers.index, inplace=True)\n",
    "    return(beer_recipes)\n",
    "\n",
    "def clean_outliers(beer_recipes):\n",
    "    outlier_cleaner(beer_recipes, 'Size(L)', 2300)\n",
    "    outlier_cleaner(beer_recipes, 'ABV', 20)\n",
    "    outlier_cleaner(beer_recipes, 'IBU', 350)\n",
    "    outlier_cleaner(beer_recipes, 'Color', 50)\n",
    "    outlier_cleaner(beer_recipes, 'BoilSize', 2000)\n",
    "    outlier_cleaner(beer_recipes, 'BoilTime', 150)\n",
    "    outlier_cleaner(beer_recipes, 'OG_sg', 1.2)\n",
    "    outlier_cleaner(beer_recipes, 'FG_sg', 1.06)\n",
    "    outlier_cleaner(beer_recipes, 'BoilGravity_sg', 1.3)\n",
    "    drop_feature_outliers = beer_recipes.loc[beer_recipes['Efficiency'] < 20]\n",
    "    beer_recipes.drop(drop_feature_outliers.index, inplace=True)\n",
    "\n",
    "    return(beer_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because some columns have a reasonable amount of useful data but too many null values to consider discarding\n",
    "# every row afflicted by them, we need to insert the median values for each type of beer into those null values\n",
    "\n",
    "def fillin_values(beer_recipes):\n",
    "    nonnumeric_feats = list(beer_recipes.select_dtypes(include=object).columns)\n",
    "\n",
    "    # separate out the nonnumeric, as they'll be removed by this process and we need to add them back in afterwards\n",
    "    beer_recipes_nonnumeric = beer_recipes[nonnumeric_feats].copy()\n",
    "    \n",
    "    # fill in NaN based on the median of the respective broad style\n",
    "    grouped = beer_recipes.groupby('BroadStyle')\n",
    "    transformed = grouped.transform(lambda x: x.fillna(x.median()))\n",
    "    beer_recipes = pd.concat([beer_recipes_nonnumeric, transformed], axis=1)\n",
    "    \n",
    "    return(beer_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to encode any non-numeric features, so that models can use them more easily\n",
    "\n",
    "def encoding_function(beer_recipes, features_list):\n",
    "    selected_data = beer_recipes.loc[:, features_list]\n",
    "    \n",
    "    categorical_features = list(selected_data.select_dtypes(include=object).columns)\n",
    "    for feature in categorical_features:\n",
    "        encoder = LabelEncoder()\n",
    "        selected_data[feature] = encoder.fit_transform(selected_data[feature])\n",
    "    return(selected_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a detailed report function that can be used for any model\n",
    "\n",
    "def accuracy_report(testing_X, testing_Y, model, cv):\n",
    "    predictions = model.predict(testing_X)\n",
    "    print('Model score:')\n",
    "    print(model.score(testing_X, testing_Y))\n",
    "    print(\" \")\n",
    "    print(\"Classification Report:\")\n",
    "    y_prediction = model.predict(testing_X)\n",
    "    print(classification_report(testing_Y, y_prediction))\n",
    "    \n",
    "# Sometimes we don't want to spend the processor time calculating the cross-valuation, so we need a way to toggle it.\n",
    "    if cv == 1:\n",
    "        print(\" \")\n",
    "        print('Model cross-valuation:')\n",
    "        print(sklearn.model_selection.cross_val_score(model, testing_X, testing_Y, cv = 5))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a report function that can give a general score for a model tested on multiple different datasets\n",
    "\n",
    "def mulifeature_accuracy_report(X_train, X_test, y_train, y_test, list_of_feature_lists, model):\n",
    "    counter = 0\n",
    "    for feature_list in list_of_feature_lists:\n",
    "        counter = counter + 1\n",
    "        selected_X_train = X_train.loc[:, feature_list]\n",
    "        selected_X_train.drop(['BroadStyle'], axis=1, inplace=True)\n",
    "        selected_X_test = X_test.loc[:, feature_list]\n",
    "        selected_X_test.drop(['BroadStyle'], axis=1, inplace=True)\n",
    "        model.fit(selected_X_train, y_train)\n",
    "        predictions = model.predict(selected_X_test)\n",
    "        print('Model score for feature list #{}: {}'.format(counter, model.score(selected_X_test, y_test)))\n",
    "        os.system('say \"all done.\"')  # this is going to take a while, let me know when it's done\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & Processing Data\n",
    "The dataframe object 'beer_recipes' will be the dataframe we'll be using all those functions on, copied from the 'basedata_beer_recipes' dataframe so that we can come back and reset beer_recipes later in the notebook without having to re-upload the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_recipes = basedata_beer_recipes.copy()\n",
    "\n",
    "beer_recipes = drop_unnecessary_columns(beer_recipes)\n",
    "beer_recipes = add_broad_styles(beer_recipes)\n",
    "beer_recipes = apply_get_sg(beer_recipes)\n",
    "beer_recipes = clean_outliers(beer_recipes)\n",
    "beer_recipes = fillin_values(beer_recipes)\n",
    "beer_recipes = beer_recipes.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beer_recipes.isnull().sum()\n",
    "#beer_recipes.describe().T\n",
    "#beer_recipes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up lists of features for experimentation\n",
    "This is necessary to make many of the functions above work properly, and especially if we want to be able to test models on multiple different sets of features within a single cell without taking up too many lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['BroadStyle', #target\n",
    "                'OG_sg','FG_sg','ABV','IBU','Color', #standardized fields\n",
    "                'BrewMethod', #categorical feature\n",
    "                'Size(L)', 'BoilSize', 'BoilTime', 'BoilGravity_sg', 'Efficiency', \n",
    "                'MashThickness', 'PitchRate', 'PrimaryTemp' # other numerical features\n",
    "                ]\n",
    "\n",
    "# top X features taken from SelectKBest, immediately below\n",
    "top_5_features = ['BroadStyle', #target\n",
    "                  'OG_sg','IBU','Color','PitchRate','PrimaryTemp' # top 5 features according to SKB\n",
    "                 ]\n",
    "\n",
    "top_7_features = ['BroadStyle', #target\n",
    "                  'OG_sg','FG_sg','IBU','ABV','Color','PitchRate','PrimaryTemp' # top 7 features according to SKB\n",
    "                 ]\n",
    "\n",
    "top_10_features = ['BroadStyle', #target\n",
    "                  'OG_sg','FG_sg','BoilGravity_sg','IBU','ABV','Color','PitchRate',\n",
    "                   'PrimaryTemp','Efficiency','MashThickness' # top 10 features according to SKB\n",
    "                 ]\n",
    "\n",
    "list_of_feature_lists = [all_features, top_10_features, top_7_features, top_5_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing SelectKBest to identify the best features\n",
    "This process was performed twice, once to identify the top 10 (listed in order, based on incrementing 'k' and noting which feature was added for each k += 1) out of the original features and once to identify the top 10 including the features added by the two PCA functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_1</th>\n",
       "      <th>best_2</th>\n",
       "      <th>best_3</th>\n",
       "      <th>best_4</th>\n",
       "      <th>best_5</th>\n",
       "      <th>best_6</th>\n",
       "      <th>best_7</th>\n",
       "      <th>best_8</th>\n",
       "      <th>best_9</th>\n",
       "      <th>best_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.055</td>\n",
       "      <td>17.650</td>\n",
       "      <td>4.830</td>\n",
       "      <td>0.750</td>\n",
       "      <td>17.780</td>\n",
       "      <td>-28.441</td>\n",
       "      <td>-7.441</td>\n",
       "      <td>-1.138</td>\n",
       "      <td>-28.368</td>\n",
       "      <td>-7.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.083</td>\n",
       "      <td>60.650</td>\n",
       "      <td>15.640</td>\n",
       "      <td>0.500</td>\n",
       "      <td>20.000</td>\n",
       "      <td>14.909</td>\n",
       "      <td>2.197</td>\n",
       "      <td>0.734</td>\n",
       "      <td>14.975</td>\n",
       "      <td>1.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.063</td>\n",
       "      <td>59.250</td>\n",
       "      <td>8.980</td>\n",
       "      <td>0.750</td>\n",
       "      <td>20.000</td>\n",
       "      <td>13.272</td>\n",
       "      <td>-4.482</td>\n",
       "      <td>0.764</td>\n",
       "      <td>13.338</td>\n",
       "      <td>-4.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.061</td>\n",
       "      <td>54.480</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>20.000</td>\n",
       "      <td>8.489</td>\n",
       "      <td>-4.825</td>\n",
       "      <td>0.799</td>\n",
       "      <td>8.555</td>\n",
       "      <td>-5.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.060</td>\n",
       "      <td>17.840</td>\n",
       "      <td>4.570</td>\n",
       "      <td>0.750</td>\n",
       "      <td>19.000</td>\n",
       "      <td>-28.272</td>\n",
       "      <td>-7.522</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-28.169</td>\n",
       "      <td>-7.878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_1  best_2  best_3  best_4  best_5  best_6  best_7  best_8  best_9  \\\n",
       "0   1.055  17.650   4.830   0.750  17.780 -28.441  -7.441  -1.138 -28.368   \n",
       "1   1.083  60.650  15.640   0.500  20.000  14.909   2.197   0.734  14.975   \n",
       "2   1.063  59.250   8.980   0.750  20.000  13.272  -4.482   0.764  13.338   \n",
       "3   1.061  54.480   8.500   0.750  20.000   8.489  -4.825   0.799   8.555   \n",
       "4   1.060  17.840   4.570   0.750  19.000 -28.272  -7.522   0.127 -28.169   \n",
       "\n",
       "   best_10  \n",
       "0   -7.648  \n",
       "1    1.937  \n",
       "2   -4.754  \n",
       "3   -5.092  \n",
       "4   -7.878  "
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking 5/10 SelectKBest features\n",
    "\n",
    "skb_data = encoding_function(beer_recipes, alldata_allpca)\n",
    "\n",
    "\n",
    "X_best = SelectKBest(f_classif, k=10).fit_transform(\n",
    "    skb_data.drop(['BroadStyle'], axis=1), \n",
    "    skb_data['BroadStyle'])\n",
    "X_best_df = pd.DataFrame({\n",
    "    'best_1':X_best[:,0],\n",
    "    'best_2':X_best[:,1],\n",
    "    'best_3':X_best[:,2],\n",
    "    'best_4':X_best[:,3],\n",
    "    'best_5':X_best[:,4],\n",
    "    'best_6':X_best[:,5],\n",
    "    'best_7':X_best[:,6],\n",
    "    'best_8':X_best[:,7],\n",
    "    'best_9':X_best[:,8],\n",
    "    'best_10':X_best[:,9]\n",
    "})\n",
    "\n",
    "X_best_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BrewMethod</th>\n",
       "      <th>BroadStyle</th>\n",
       "      <th>Size(L)</th>\n",
       "      <th>ABV</th>\n",
       "      <th>IBU</th>\n",
       "      <th>Color</th>\n",
       "      <th>BoilSize</th>\n",
       "      <th>BoilTime</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>MashThickness</th>\n",
       "      <th>PitchRate</th>\n",
       "      <th>PrimaryTemp</th>\n",
       "      <th>OG_sg</th>\n",
       "      <th>FG_sg</th>\n",
       "      <th>BoilGravity_sg</th>\n",
       "      <th>top_10_pca_1</th>\n",
       "      <th>top_10_pca_2</th>\n",
       "      <th>top_10_pca_3</th>\n",
       "      <th>top_10_pca_4</th>\n",
       "      <th>top_10_pca_5</th>\n",
       "      <th>properties_pca_1</th>\n",
       "      <th>properties_pca_2</th>\n",
       "      <th>properties_pca_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BeerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All Grain</td>\n",
       "      <td>Ale</td>\n",
       "      <td>21.770</td>\n",
       "      <td>5.480</td>\n",
       "      <td>17.650</td>\n",
       "      <td>4.830</td>\n",
       "      <td>28.390</td>\n",
       "      <td>75</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>17.780</td>\n",
       "      <td>1.055</td>\n",
       "      <td>1.013</td>\n",
       "      <td>1.038</td>\n",
       "      <td>-28.441</td>\n",
       "      <td>-3.503</td>\n",
       "      <td>-7.441</td>\n",
       "      <td>-1.138</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-28.368</td>\n",
       "      <td>-7.648</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Grain</td>\n",
       "      <td>Winter Beer</td>\n",
       "      <td>20.820</td>\n",
       "      <td>8.160</td>\n",
       "      <td>60.650</td>\n",
       "      <td>15.640</td>\n",
       "      <td>24.610</td>\n",
       "      <td>60</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.083</td>\n",
       "      <td>1.021</td>\n",
       "      <td>1.070</td>\n",
       "      <td>14.909</td>\n",
       "      <td>-3.704</td>\n",
       "      <td>2.197</td>\n",
       "      <td>0.734</td>\n",
       "      <td>-1.670</td>\n",
       "      <td>14.975</td>\n",
       "      <td>1.937</td>\n",
       "      <td>1.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>extract</td>\n",
       "      <td>IPA</td>\n",
       "      <td>18.930</td>\n",
       "      <td>5.910</td>\n",
       "      <td>59.250</td>\n",
       "      <td>8.980</td>\n",
       "      <td>22.710</td>\n",
       "      <td>60</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.018</td>\n",
       "      <td>1.052</td>\n",
       "      <td>13.272</td>\n",
       "      <td>-4.097</td>\n",
       "      <td>-4.482</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.212</td>\n",
       "      <td>13.338</td>\n",
       "      <td>-4.754</td>\n",
       "      <td>-0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All Grain</td>\n",
       "      <td>IPA</td>\n",
       "      <td>22.710</td>\n",
       "      <td>5.800</td>\n",
       "      <td>54.480</td>\n",
       "      <td>8.500</td>\n",
       "      <td>26.500</td>\n",
       "      <td>60</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>20.000</td>\n",
       "      <td>1.061</td>\n",
       "      <td>1.017</td>\n",
       "      <td>1.052</td>\n",
       "      <td>8.489</td>\n",
       "      <td>-4.026</td>\n",
       "      <td>-4.825</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.222</td>\n",
       "      <td>8.555</td>\n",
       "      <td>-5.092</td>\n",
       "      <td>-0.309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All Grain</td>\n",
       "      <td>Ale</td>\n",
       "      <td>50.000</td>\n",
       "      <td>6.480</td>\n",
       "      <td>17.840</td>\n",
       "      <td>4.570</td>\n",
       "      <td>60.000</td>\n",
       "      <td>90</td>\n",
       "      <td>72.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>19.000</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.010</td>\n",
       "      <td>1.050</td>\n",
       "      <td>-28.272</td>\n",
       "      <td>-5.513</td>\n",
       "      <td>-7.522</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-1.170</td>\n",
       "      <td>-28.169</td>\n",
       "      <td>-7.878</td>\n",
       "      <td>1.119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       BrewMethod   BroadStyle  Size(L)   ABV    IBU  Color  BoilSize  \\\n",
       "BeerID                                                                  \n",
       "1       All Grain          Ale   21.770 5.480 17.650  4.830    28.390   \n",
       "2       All Grain  Winter Beer   20.820 8.160 60.650 15.640    24.610   \n",
       "3         extract          IPA   18.930 5.910 59.250  8.980    22.710   \n",
       "4       All Grain          IPA   22.710 5.800 54.480  8.500    26.500   \n",
       "5       All Grain          Ale   50.000 6.480 17.840  4.570    60.000   \n",
       "\n",
       "        BoilTime  Efficiency  MashThickness  PitchRate  PrimaryTemp  OG_sg  \\\n",
       "BeerID                                                                       \n",
       "1             75      70.000          1.500      0.750       17.780  1.055   \n",
       "2             60      70.000          1.500      0.500       20.000  1.083   \n",
       "3             60      70.000          1.500      0.750       20.000  1.063   \n",
       "4             60      70.000          1.500      0.750       20.000  1.061   \n",
       "5             90      72.000          1.500      0.750       19.000  1.060   \n",
       "\n",
       "        FG_sg  BoilGravity_sg  top_10_pca_1  top_10_pca_2  top_10_pca_3  \\\n",
       "BeerID                                                                    \n",
       "1       1.013           1.038       -28.441        -3.503        -7.441   \n",
       "2       1.021           1.070        14.909        -3.704         2.197   \n",
       "3       1.018           1.052        13.272        -4.097        -4.482   \n",
       "4       1.017           1.052         8.489        -4.026        -4.825   \n",
       "5       1.010           1.050       -28.272        -5.513        -7.522   \n",
       "\n",
       "        top_10_pca_4  top_10_pca_5  properties_pca_1  properties_pca_2  \\\n",
       "BeerID                                                                   \n",
       "1             -1.138        -0.253           -28.368            -7.648   \n",
       "2              0.734        -1.670            14.975             1.937   \n",
       "3              0.764         0.212            13.338            -4.754   \n",
       "4              0.799         0.222             8.555            -5.092   \n",
       "5              0.127        -1.170           -28.169            -7.878   \n",
       "\n",
       "        properties_pca_3  \n",
       "BeerID                    \n",
       "1                  0.114  \n",
       "2                  1.688  \n",
       "3                 -0.295  \n",
       "4                 -0.309  \n",
       "5                  1.119  "
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beer_recipes.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for top features out of the original set were:\n",
    "1. Color\n",
    "2. PitchRate\n",
    "3. IBU\n",
    "4. PrimaryTemp\n",
    "5. OG_sg\n",
    "6. ABV\n",
    "7. FG_sg\n",
    "8. BoilGravity_sg\n",
    "9. Efficiency\n",
    "10. MashThickness\n",
    "\n",
    "Results for top features out of everything including the two PCAs were:\n",
    "1. properties_pca_2\n",
    "2. Color\n",
    "3. top_10_pca_3\n",
    "4. PitchRate\n",
    "5. properties_pca_1\n",
    "6. IBU\n",
    "7. top_10_pca_1\n",
    "8. PrimaryTemp\n",
    "9. top_10_pca_4\n",
    "10. OG_sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PCA features\n",
    "My approach here is to make a PCA that takes the top 10 features and squishes them down into 5 PCA features, and then to go more focused and create 3 PCA features based on six of the more-highly-correlated properties like color, IBU, ABV, and the specific gravity trio.  Neither on its own turns out to improve the models hugely, but the properties-based PCAs do seem to increase performance by a couple of percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_engineering_1(beer_recipes):\n",
    "\n",
    "    # calculate PCA based on top 10 features\n",
    "    features_to_pca = beer_recipes[top_10_features]\n",
    "    features_to_pca.drop(['BroadStyle'], axis=1, inplace=True)\n",
    "    pca = PCA(n_components=5)\n",
    "    top_10_pca = pca.fit_transform(features_to_pca)\n",
    "\n",
    "    # join the PCAs up with the input dataframe\n",
    "    beer_recipes['top_10_pca_1'] = top_10_pca[:,0]\n",
    "    beer_recipes['top_10_pca_2'] = top_10_pca[:,1]\n",
    "    beer_recipes['top_10_pca_3'] = top_10_pca[:,2]\n",
    "    beer_recipes['top_10_pca_4'] = top_10_pca[:,3]\n",
    "    beer_recipes['top_10_pca_5'] = top_10_pca[:,4] \n",
    "    \n",
    "    return(beer_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_engineering_2(beer_recipes):\n",
    "    features = ['OG_sg','FG_sg','BoilGravity_sg','IBU','ABV','Color']\n",
    "    # calculate PCA based on the more-correlated variables out of the top 10\n",
    "    features_to_pca = beer_recipes[features]\n",
    "    pca = PCA(n_components=3)\n",
    "    properties_pca = pca.fit_transform(features_to_pca)\n",
    "\n",
    "    # join the PCAs up with the input dataframe\n",
    "    beer_recipes['properties_pca_1'] = properties_pca[:,0]\n",
    "    beer_recipes['properties_pca_2'] = properties_pca[:,1]\n",
    "    beer_recipes['properties_pca_3'] = properties_pca[:,2]\n",
    "    \n",
    "    return(beer_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature lists for the PCA features\n",
    "\n",
    "top_10_pca = ['BroadStyle', #target\n",
    "              'top_10_pca_1','top_10_pca_2','top_10_pca_3','top_10_pca_4','top_10_pca_5'\n",
    "             ]\n",
    "\n",
    "alldata_top10pca = ['BroadStyle', #target\n",
    "                    'OG_sg','FG_sg','ABV','IBU','Color', #standardized fields\n",
    "                    'BrewMethod', #categorical feature\n",
    "                    'Size(L)', 'BoilSize', 'BoilTime', 'BoilGravity_sg', 'Efficiency', \n",
    "                    'MashThickness', 'PitchRate', 'PrimaryTemp', # other numerical features\n",
    "                    'top_10_pca_1','top_10_pca_2','top_10_pca_3','top_10_pca_4','top_10_pca_5',\n",
    "                   ] \n",
    "\n",
    "alldata_properties_pca = ['BroadStyle', #target\n",
    "                    'OG_sg','FG_sg','ABV','IBU','Color', #standardized fields\n",
    "                    'BrewMethod', #categorical feature\n",
    "                    'Size(L)', 'BoilSize', 'BoilTime', 'BoilGravity_sg', 'Efficiency', \n",
    "                    'MashThickness', 'PitchRate', 'PrimaryTemp', # other numerical features\n",
    "                    'properties_pca_1','properties_pca_2','properties_pca_3'\n",
    "                   ] \n",
    "\n",
    "skb10_with_pca = ['BroadStyle', #target\n",
    "                  'properties_pca_2', 'Color', 'top_10_pca_3', 'PitchRate', 'properties_pca_1',\n",
    "                  'IBU', 'top_10_pca_1', 'PrimaryTemp', 'top_10_pca_4', 'OG_sg'\n",
    "                 ]\n",
    "\n",
    "alldata_allpca = ['BroadStyle', #target\n",
    "                    'OG_sg','FG_sg','ABV','IBU','Color', #standardized fields\n",
    "                    'BrewMethod', #categorical feature\n",
    "                    'Size(L)', 'BoilSize', 'BoilTime', 'BoilGravity_sg', 'Efficiency', \n",
    "                    'MashThickness', 'PitchRate', 'PrimaryTemp', # other numerical features\n",
    "                    'top_10_pca_1','top_10_pca_2','top_10_pca_3','top_10_pca_4','top_10_pca_5',\n",
    "                    'properties_pca_1','properties_pca_2','properties_pca_3'\n",
    "                   ] \n",
    "\n",
    "# update list_of_feature_lists  alldata_top10pca, \n",
    "list_of_feature_lists = [all_features, top_10_features, top_7_features, top_5_features, \n",
    "                         alldata_allpca, top_10_pca, alldata_top10pca, alldata_properties_pca, skb10_with_pca]\n",
    "feature_list_str = ['all_features', 'top_10_features', 'top_7_features', 'top_5_features', \n",
    "                    'alldata_allpca', 'top_10_pca', 'alldata_top10pca', 'alldata_properties_pca', 'skb10_with_pca']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "# Establish the new version of beer_recipes\n",
    "beer_recipes = basedata_beer_recipes.copy()\n",
    "\n",
    "# do all the same preprocessing to it (using the new version of add_broad_styles_v2)\n",
    "beer_recipes = drop_unnecessary_columns(beer_recipes)\n",
    "beer_recipes = add_broad_styles_v2(beer_recipes) # <- this is the new version\n",
    "beer_recipes = apply_get_sg(beer_recipes)\n",
    "beer_recipes = clean_outliers(beer_recipes)\n",
    "beer_recipes = fillin_values(beer_recipes)\n",
    "beer_recipes = beer_recipes.dropna()\n",
    "\n",
    "# run PCA function\n",
    "beer_recipes = PCA_engineering_1(beer_recipes)\n",
    "beer_recipes = PCA_engineering_2(beer_recipes)\n",
    "\n",
    "# Select the feature list to be used and encode categorical data with the encoding function\n",
    "data_to_model = encoding_function(beer_recipes, alldata_allpca)\n",
    "\n",
    "# reset the encoder so that it can be used to reverse-engineer the broad_styles afterwards\n",
    "encoder = LabelEncoder()\n",
    "encoded_broadtypes = encoder.fit_transform(beer_recipes['BroadStyle'])\n",
    "\n",
    "os.system('say \"all done.\"'); print('\\a')  # this is going to take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling data for faster run-times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BroadStyle</th>\n",
       "      <th>OG_sg</th>\n",
       "      <th>FG_sg</th>\n",
       "      <th>ABV</th>\n",
       "      <th>IBU</th>\n",
       "      <th>Color</th>\n",
       "      <th>BrewMethod</th>\n",
       "      <th>Size(L)</th>\n",
       "      <th>BoilSize</th>\n",
       "      <th>BoilTime</th>\n",
       "      <th>BoilGravity_sg</th>\n",
       "      <th>Efficiency</th>\n",
       "      <th>MashThickness</th>\n",
       "      <th>PitchRate</th>\n",
       "      <th>PrimaryTemp</th>\n",
       "      <th>top_10_pca_1</th>\n",
       "      <th>top_10_pca_2</th>\n",
       "      <th>top_10_pca_3</th>\n",
       "      <th>top_10_pca_4</th>\n",
       "      <th>top_10_pca_5</th>\n",
       "      <th>properties_pca_1</th>\n",
       "      <th>properties_pca_2</th>\n",
       "      <th>properties_pca_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BeerID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.055</td>\n",
       "      <td>1.013</td>\n",
       "      <td>5.480</td>\n",
       "      <td>17.650</td>\n",
       "      <td>4.830</td>\n",
       "      <td>0</td>\n",
       "      <td>21.770</td>\n",
       "      <td>28.390</td>\n",
       "      <td>75</td>\n",
       "      <td>1.038</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>17.780</td>\n",
       "      <td>-28.783</td>\n",
       "      <td>-3.511</td>\n",
       "      <td>-7.437</td>\n",
       "      <td>-1.151</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-28.710</td>\n",
       "      <td>-7.673</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.063</td>\n",
       "      <td>1.018</td>\n",
       "      <td>5.910</td>\n",
       "      <td>59.250</td>\n",
       "      <td>8.980</td>\n",
       "      <td>3</td>\n",
       "      <td>18.930</td>\n",
       "      <td>22.710</td>\n",
       "      <td>60</td>\n",
       "      <td>1.052</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>20.000</td>\n",
       "      <td>12.925</td>\n",
       "      <td>-4.102</td>\n",
       "      <td>-4.429</td>\n",
       "      <td>0.757</td>\n",
       "      <td>-0.238</td>\n",
       "      <td>12.994</td>\n",
       "      <td>-4.735</td>\n",
       "      <td>-0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.061</td>\n",
       "      <td>1.017</td>\n",
       "      <td>5.800</td>\n",
       "      <td>54.480</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0</td>\n",
       "      <td>22.710</td>\n",
       "      <td>26.500</td>\n",
       "      <td>60</td>\n",
       "      <td>1.052</td>\n",
       "      <td>70.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>20.000</td>\n",
       "      <td>8.143</td>\n",
       "      <td>-4.031</td>\n",
       "      <td>-4.778</td>\n",
       "      <td>0.791</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>8.211</td>\n",
       "      <td>-5.079</td>\n",
       "      <td>-0.303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1.060</td>\n",
       "      <td>1.010</td>\n",
       "      <td>6.480</td>\n",
       "      <td>17.840</td>\n",
       "      <td>4.570</td>\n",
       "      <td>0</td>\n",
       "      <td>50.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>90</td>\n",
       "      <td>1.050</td>\n",
       "      <td>72.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.750</td>\n",
       "      <td>19.000</td>\n",
       "      <td>-28.616</td>\n",
       "      <td>-5.522</td>\n",
       "      <td>-7.501</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1.167</td>\n",
       "      <td>-28.509</td>\n",
       "      <td>-7.903</td>\n",
       "      <td>1.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>1.055</td>\n",
       "      <td>1.013</td>\n",
       "      <td>5.580</td>\n",
       "      <td>40.120</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0</td>\n",
       "      <td>24.610</td>\n",
       "      <td>29.340</td>\n",
       "      <td>70</td>\n",
       "      <td>1.047</td>\n",
       "      <td>79.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>-6.406</td>\n",
       "      <td>-12.718</td>\n",
       "      <td>-4.199</td>\n",
       "      <td>0.981</td>\n",
       "      <td>-0.282</td>\n",
       "      <td>-6.159</td>\n",
       "      <td>-5.164</td>\n",
       "      <td>-0.265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        BroadStyle  OG_sg  FG_sg   ABV    IBU  Color  BrewMethod  Size(L)  \\\n",
       "BeerID                                                                      \n",
       "1                0  1.055  1.013 5.480 17.650  4.830           0   21.770   \n",
       "3                5  1.063  1.018 5.910 59.250  8.980           3   18.930   \n",
       "4                5  1.061  1.017 5.800 54.480  8.500           0   22.710   \n",
       "5                0  1.060  1.010 6.480 17.840  4.570           0   50.000   \n",
       "6                9  1.055  1.013 5.580 40.120  8.000           0   24.610   \n",
       "\n",
       "        BoilSize  BoilTime  BoilGravity_sg  Efficiency  MashThickness  \\\n",
       "BeerID                                                                  \n",
       "1         28.390        75           1.038      70.000          1.500   \n",
       "3         22.710        60           1.052      70.000          1.500   \n",
       "4         26.500        60           1.052      70.000          1.500   \n",
       "5         60.000        90           1.050      72.000          1.500   \n",
       "6         29.340        70           1.047      79.000          1.500   \n",
       "\n",
       "        PitchRate  PrimaryTemp  top_10_pca_1  top_10_pca_2  top_10_pca_3  \\\n",
       "BeerID                                                                     \n",
       "1           0.750       17.780       -28.783        -3.511        -7.437   \n",
       "3           0.750       20.000        12.925        -4.102        -4.429   \n",
       "4           0.750       20.000         8.143        -4.031        -4.778   \n",
       "5           0.750       19.000       -28.616        -5.522        -7.501   \n",
       "6           1.000       20.000        -6.406       -12.718        -4.199   \n",
       "\n",
       "        top_10_pca_4  top_10_pca_5  properties_pca_1  properties_pca_2  \\\n",
       "BeerID                                                                   \n",
       "1             -1.151         0.233           -28.710            -7.673   \n",
       "3              0.757        -0.238            12.994            -4.735   \n",
       "4              0.791        -0.248             8.211            -5.079   \n",
       "5              0.113         1.167           -28.509            -7.903   \n",
       "6              0.981        -0.282            -6.159            -5.164   \n",
       "\n",
       "        properties_pca_3  \n",
       "BeerID                    \n",
       "1                  0.126  \n",
       "3                 -0.290  \n",
       "4                 -0.303  \n",
       "5                  1.131  \n",
       "6                 -0.265  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "sample_to_model = data_to_model.sample(frac=.2, random_state=2, axis=0)\n",
    "\n",
    "X = sample_to_model.iloc[:, 1:]\n",
    "y = sample_to_model.iloc[:, 0] #the target was the first column included in features_list\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, stratify=y, random_state=35)\n",
    "\n",
    "os.system('say \"all done.\"'); print('\\a')  # this is going to take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running multiple different feature-sets to determine which is optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier, multiple feature-list test series:\n",
      "Model score for feature list #1: 0.6378077839555203\n",
      "Model score for feature list #2: 0.6409849086576648\n",
      "Model score for feature list #3: 0.6505162827640985\n",
      "Model score for feature list #4: 0.625099285146942\n",
      "Model score for feature list #5: 0.6378077839555203\n",
      "Model score for feature list #6: 0.6020651310563939\n",
      "Model score for feature list #7: 0.6282764098490866\n",
      "Model score for feature list #8: 0.6481334392374901\n",
      "Model score for feature list #9: 0.6195393169181891\n"
     ]
    }
   ],
   "source": [
    "# Random Forest model\n",
    "rfc = RandomForestClassifier()\n",
    "print('Random Forest Classifier, multiple feature-list test series:')\n",
    "mulifeature_accuracy_report(X_train, X_test, y_train, y_test, list_of_feature_lists, rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classifier, multiple feature-list test series:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #1: 0.6211278792692613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #2: 0.6354249404289118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #3: 0.6401906274821286\n",
      "Model score for feature list #4: 0.6195393169181891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #5: 0.5988880063542494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #6: 0.6187450357426529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #7: 0.6187450357426529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #8: 0.6179507545671168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score for feature list #9: 0.613979348689436\n"
     ]
    }
   ],
   "source": [
    "# MLP model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,200,100,100),\n",
    "                    activation= 'logistic', \n",
    "                    solver='adam', \n",
    "                    alpha=0.0001, \n",
    "                    batch_size='auto', \n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.001, \n",
    "                    power_t=0.5, \n",
    "                    max_iter=400, \n",
    "                    shuffle=True, \n",
    "                    random_state=105, \n",
    "                    tol=0.0001, \n",
    "                    verbose=False, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, \n",
    "                    nesterovs_momentum=True, \n",
    "                    early_stopping=False, \n",
    "                    validation_fraction=0.1, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08, \n",
    "                    n_iter_no_change=10)\n",
    "print('MLP Classifier, multiple feature-list test series:')\n",
    "mulifeature_accuracy_report(X_train, X_test, y_train, y_test, list_of_feature_lists, mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a version 2 of the add_broad_styles function, one which removes the offending broad styles\n",
    "def add_broad_styles_v2(beer_recipes):\n",
    "    broad_styles = ['Ale', 'IPA', 'Pale Ale', 'Lager', 'Stout', \n",
    "                    'Bitter', 'Porter', 'Mead', 'Pilsner', \n",
    "                    'Weizen', 'Saison', 'Kölsch', 'American Wheat', \n",
    "                    'Barleywine', 'English Brown Ale']\n",
    "    beer_recipes['BroadStyle'] = 'Other'\n",
    "    beer_recipes['Style'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    for broad_style in broad_styles:\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains(broad_style), 'BroadStyle'] = broad_style\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Pils'), 'BroadStyle'] = 'Pilsner'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Melomel'), 'BroadStyle'] = 'Mead'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Metheglin'), 'BroadStyle'] = 'Mead'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Witbier'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Weiss'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('weiz'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Gose'), 'BroadStyle'] = 'Weizen'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Schwarzbier'), 'BroadStyle'] = 'Lager'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('bock'), 'BroadStyle'] = 'Bock'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Tripel'), 'BroadStyle'] = 'Pale Ale'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Bock'), 'BroadStyle'] = 'Bock'\n",
    "        beer_recipes.loc[beer_recipes['Style'].str.contains('Porter'), 'BroadStyle'] = 'Stout'\n",
    "    \n",
    "    drop_Other_style = beer_recipes.loc[beer_recipes['BroadStyle'] == 'Other']\n",
    "    beer_recipes.drop(drop_Other_style.index, inplace=True)\n",
    "\n",
    "    beer_recipes.drop(['Style'], axis=1, inplace=True)\n",
    "\n",
    "    return(beer_recipes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup comparison runs between RFC and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "# Establish the new version of beer_recipes\n",
    "beer_recipes = basedata_beer_recipes.copy()\n",
    "\n",
    "# do all the same preprocessing to it (using the new version of add_broad_styles_v2)\n",
    "beer_recipes = drop_unnecessary_columns(beer_recipes)\n",
    "beer_recipes = add_broad_styles_v2(beer_recipes) # <- this is the new version\n",
    "beer_recipes = apply_get_sg(beer_recipes)\n",
    "beer_recipes = clean_outliers(beer_recipes)\n",
    "beer_recipes = fillin_values(beer_recipes)\n",
    "beer_recipes = beer_recipes.dropna()\n",
    "\n",
    "# run PCA function\n",
    "beer_recipes = PCA_engineering_1(beer_recipes)\n",
    "beer_recipes = PCA_engineering_2(beer_recipes)\n",
    "\n",
    "# Select the feature list to be used and encode categorical data with the encoding function\n",
    "data_to_model = encoding_function(beer_recipes, top_7_features)\n",
    "\n",
    "# reset the encoder so that it can be used to reverse-engineer the broad_styles afterwards\n",
    "encoder = LabelEncoder()\n",
    "encoded_broadtypes = encoder.fit_transform(beer_recipes['BroadStyle'])\n",
    "\n",
    "X = data_to_model.iloc[:, 1:]\n",
    "y = data_to_model.iloc[:, 0] #the target was the first column included in features_list\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=35)\n",
    "\n",
    "os.system('say \"all done.\"'); print('\\a')  # this is going to take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling data for faster run-times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "sample_to_model = data_to_model.sample(frac=.25, random_state=2, axis=0)\n",
    "\n",
    "X = sample_to_model.iloc[:, 1:]\n",
    "y = sample_to_model.iloc[:, 0] #the target was the first column included in features_list\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, stratify=y, random_state=35)\n",
    "\n",
    "os.system('say \"all done.\"'); print('\\a')  # this is going to take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC results:\n",
      " \n",
      "Model score:\n",
      "0.6442185514612452\n",
      " \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.55      0.53       529\n",
      "           1       0.31      0.22      0.26        63\n",
      "           2       0.68      0.72      0.70        29\n",
      "           3       0.46      0.33      0.38       122\n",
      "           4       0.76      0.55      0.64        40\n",
      "           5       0.74      0.79      0.76       855\n",
      "           6       0.55      0.36      0.44        44\n",
      "           7       0.73      0.76      0.75       207\n",
      "           8       0.71      0.45      0.56        11\n",
      "           9       0.49      0.49      0.49       432\n",
      "          10       0.53      0.49      0.51        63\n",
      "          11       0.60      0.47      0.53       131\n",
      "          12       0.83      0.86      0.85       424\n",
      "          13       0.62      0.60      0.61       198\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      3148\n",
      "   macro avg       0.61      0.55      0.57      3148\n",
      "weighted avg       0.64      0.64      0.64      3148\n",
      "\n",
      " \n",
      "Model cross-valuation:\n",
      "[0.63937008 0.65402844 0.62539683 0.64593301 0.66934189]\n",
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "print('RFC results:')\n",
    "print(' ')\n",
    "accuracy_report(X_test, y_test, rfc, 1)\n",
    "os.system('say \"all done.\"'); print('\\a')  # this is going to take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "Create a multi-layer perceptron neural network model to predict on a labeled dataset of your choosing. Compare this model to either a boosted tree or a random forest model and describe the relative tradeoffs between complexity and accuracy. Be sure to vary the hyperparameters of your MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Establish and fit the model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(200,150,100,50,25),\n",
    "                    activation= 'logistic', \n",
    "                    solver='adam', \n",
    "                    alpha=0.0001, \n",
    "                    batch_size='auto', \n",
    "                    learning_rate='constant', \n",
    "                    learning_rate_init=0.001, \n",
    "                    power_t=0.5, \n",
    "                    max_iter=500, \n",
    "                    shuffle=True, \n",
    "                    random_state=105, \n",
    "                    tol=0.0001, \n",
    "                    verbose=False, \n",
    "                    warm_start=False, \n",
    "                    momentum=0.9, \n",
    "                    nesterovs_momentum=True, \n",
    "                    early_stopping=False, \n",
    "                    validation_fraction=0.1, \n",
    "                    beta_1=0.9, \n",
    "                    beta_2=0.999, \n",
    "                    epsilon=1e-08, \n",
    "                    n_iter_no_change=10)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score:\n",
      "0.6550190597204575\n",
      " \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.52      0.53       529\n",
      "           1       0.28      0.24      0.26        63\n",
      "           2       0.70      0.55      0.62        29\n",
      "           3       0.44      0.25      0.32       122\n",
      "           4       0.70      0.57      0.63        40\n",
      "           5       0.76      0.81      0.78       855\n",
      "           6       0.38      0.52      0.44        44\n",
      "           7       0.74      0.71      0.73       207\n",
      "           8       0.70      0.64      0.67        11\n",
      "           9       0.52      0.55      0.53       432\n",
      "          10       0.59      0.57      0.58        63\n",
      "          11       0.57      0.50      0.53       131\n",
      "          12       0.84      0.87      0.86       424\n",
      "          13       0.57      0.64      0.60       198\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      3148\n",
      "   macro avg       0.60      0.57      0.58      3148\n",
      "weighted avg       0.65      0.66      0.65      3148\n",
      "\n",
      " \n",
      "Model cross-valuation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61417323 0.60663507 0.59206349 0.63476874 0.63402889]\n"
     ]
    }
   ],
   "source": [
    "accuracy_report(X_test, y_test, mlp, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
