{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "import math\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "#from nltk.corpus import gutenberg, stopwords\n",
    "#from collections import Counter\n",
    "#import nltk\n",
    "#nltk.download('gutenberg')\n",
    "\n",
    "#from spacy.lang.en import English\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import urllib.request\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from itertools import cycle\n",
    "\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-page scraper\n",
    "Below is a spider that will go through a forum page with 100 threads, collect the urls for each thread, and then go to each thread's first page and collect the title, url, first post's text, original posting date, and how many posts were in the thread.  Note that, for some reason, the threads in the JSON document are NOT in the order they appear on the forum page..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class PFforumSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"PFforum\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://paizo.com/community/forums/pathfinder/pathfinderRPG/general',\n",
    "    ]\n",
    "\n",
    "    # set up a parse to go through and collect all the thread urls, then open them one by one and scrape their data\n",
    "    def parse(self, response):\n",
    "        for href in response.xpath(\"//table/tbody[//b]/tr/td/a[b]/@href\").extract():\n",
    "            url = response.urljoin(href)\n",
    "            yield scrapy.Request(url, callback = self.parse_dir_contents)\n",
    "\n",
    "    # set up a parse to scrape the threads' individual data \n",
    "    # example link: https://paizo.com/threads/rzs2k2y8?Best-Open-Content\n",
    "    def parse_dir_contents(self, response):\n",
    "        for sel in response.xpath('//td[@id=\"main-slot\"]'):\n",
    "            yield {\n",
    "                'title': sel.xpath('//h1[a]/a/text()').extract(),\n",
    "                'link': sel.xpath('//h1[a]/a/@href').extract(),\n",
    "                'first_post': sel.xpath(\"normalize-space(string(//*[@itemprop='commentText']))\").extract(),\n",
    "                'first_post_date': sel.xpath('//span/a/time/text()').extract_first(),\n",
    "                'thread_length': sel.xpath(\"normalize-space(string(//td[@class='tiny']))\").extract()\n",
    "                # Note that \"thread_length\" will be null if it's less than 51, as the marker the xpath expression\n",
    "                # is reading doesn't appear unless there are more than 50 comments in the thread\n",
    "            }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'pf_forum_scrape.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False,           # Turn off logging for now.\n",
    "    'ROBOTSTXT_OBEY': True,\n",
    "    'USER_AGENT': 'ThinkfulDataScienceBootcampCrawler (thinkful.com)',\n",
    "    'AUTOTHROTTLE_ENABLED': True,\n",
    "    'HTTPCACHE_ENABLED': True\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(PFforumSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "                                          first_post  \\\n",
      "0  [Hey guys itâ€™s gonna be my first time being a ...   \n",
      "1  [Really. it can do eveyrthing. with the human/...   \n",
      "2  [In the Bible the original Horseman Of Famine ...   \n",
      "3  [I think this has come up in almost every grou...   \n",
      "4  [What are some rules appropriate ways to allow...   \n",
      "\n",
      "               first_post_date  \\\n",
      "0  Wed, Apr 24, 2019, 01:13 pm   \n",
      "1  Tue, Apr 23, 2019, 04:13 pm   \n",
      "2  Thu, Apr 25, 2019, 01:19 pm   \n",
      "3  Fri, Apr 12, 2019, 01:05 pm   \n",
      "4   Mon, Apr 8, 2019, 02:16 pm   \n",
      "\n",
      "                                                link    thread_length  \\\n",
      "0  [https://paizo.com/threads/rzs42jji?Making-a-m...               []   \n",
      "1  [https://paizo.com/threads/rzs42jii?Is-it-me-o...  [1 to 50 of 56]   \n",
      "2  [https://paizo.com/threads/rzs42jkm?Scales-for...               []   \n",
      "3  [https://paizo.com/threads/rzs42j5s?How-do-you...               []   \n",
      "4  [https://paizo.com/threads/rzs42j0z?Making-a-v...  [1 to 50 of 60]   \n",
      "\n",
      "                                               title  \n",
      "0                        [Making a monster campaign]  \n",
      "1  [Is it me, or is Shaman the best class in the ...  \n",
      "2                         [Scales for Trelmarixian?]  \n",
      "3  [How do you handle high explosives? (Player cr...  \n",
      "4          [Making a villain monologue possible...?]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "firstpage = pd.read_json('pf_forum_scrape.json')\n",
    "print(firstpage.shape)\n",
    "print(firstpage.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://paizo.com/threads/rzs42j5s?How-do-you-handle-high-explosives']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstpage.loc[3,'link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single-page scraper\n",
    "\n",
    "Below is a spider that will scrape the main forum page for titles, links, original post dates, and latest post dates for each thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "\n",
    "class PFforumSpider(scrapy.Spider):\n",
    "    # Naming the spider is important if you are running more than one spider of\n",
    "    # this class simultaneously.\n",
    "    name = \"PFforum\"\n",
    "    \n",
    "    # URL(s) to start with.\n",
    "    start_urls = [\n",
    "        'https://paizo.com/community/forums/pathfinder/pathfinderRPG/general',\n",
    "    ]\n",
    "            \n",
    "    # Use XPath to parse the response we get.\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Iterate over every <article> element on the page.\n",
    "        for thread in response.xpath('//table/tbody[//b]'):\n",
    "\n",
    "            # Yield a dictionary with the values we want.\n",
    "            yield {\n",
    "                # This is the code to choose what we want to extract\n",
    "                # You can modify this with other Xpath expressions to extract other information from the site\n",
    "                'thread_title': thread.xpath('tr/td/a/b/text()').extract_first(),\n",
    "                'link': thread.xpath('tr/td/a[b]/@href').extract_first(),\n",
    "                'original_post_date': thread.xpath('tr/td/div/span/time/text()').extract_first(),\n",
    "                'newest_post_date': thread.xpath('tr/td/a/time/text()').extract_first()\n",
    "                }\n",
    "\n",
    "# Tell the script how to run the crawler by passing in settings.\n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',         # Store data in JSON format.\n",
    "    'FEED_URI': 'pf_forum_firstpage.json',  # Name our storage file.\n",
    "    'LOG_ENABLED': False           # Turn off logging for now.\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(PFforumSpider)\n",
    "process.start()\n",
    "print('Success!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "                                                link  \\\n",
      "0  https://paizo.com/threads/rzs2k2y8?Best-Open-C...   \n",
      "1      https://paizo.com/threads/rzs2l3b0?FAQ-System   \n",
      "2  https://paizo.com/threads/rzs42jqj?What-is-you...   \n",
      "3     https://paizo.com/threads/rzs42jok?Blood-Hexes   \n",
      "4  https://paizo.com/threads/rzs42jnt?SoVigilante...   \n",
      "\n",
      "                                                name        newest_post_date  \\\n",
      "0                                  Best Open Content   Mar 6, 2018, 05:38 pm   \n",
      "1                                         FAQ System  Feb 17, 2018, 08:18 am   \n",
      "2         What is your top 10-20 low level monsters?          12 minutes ago   \n",
      "3                                        Blood Hexes          30 minutes ago   \n",
      "4  So..Vigilante. Can someone explain to me how i...   1 hour, 7 minutes ago   \n",
      "\n",
      "       original_post_date  \n",
      "0  Oct 23, 2009, 02:38 pm  \n",
      "1  Jul 28, 2010, 12:42 pm  \n",
      "2          12 minutes ago  \n",
      "3        Sunday, 06:19 pm  \n",
      "4      Saturday, 06:45 pm  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "firstpage = pd.read_json('pf_forum_firstpage.json')\n",
    "print(firstpage.shape)\n",
    "print(firstpage.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
