{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make five implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Working on the https://www.kaggle.com/moltean/fruits/ image-recognition dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "CNN seems by far the best for this particular dataset.  It takes quite a while to process, especially on my meagre laptop, but it's really hard to argue with a 98% validation success rate.  Interestingly, it was the first two iterations I tried (\"Convolutional Neural Network, v1\" and \"Convolutional Neural Network, v2\") that produced the best and most reliable results, with val_acc scores of 98.49% and 98.58%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq MLP Conclusions:\n",
    "Decent, and very fast (once you've uploaded the images) but not in the 90%+ category yet. \n",
    "\n",
    "Extra Epochs seems to do little, as by #10 it seems to have reached a point of minimal or negative returns, where the random associations in each iteration cause it to \"forget\" how to label some fruits even as it learns how to label others.\n",
    "\n",
    "The most critical factor seems to be the image size, as a scale of 100 pixels (the scale of the images in the download for this collection) produces abysmal results, and gets progressively better until it reaches somewhere between 10 and 20, after which further decreases drop performance (likely due to simply not having enough identifying data to go on in each image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN conclusions:\n",
    "The most effective of the three methods, though also the most time-intensive.\n",
    "\n",
    "Increasing the kernel size seemed to reduce performance.  Optimal image size was somewhere in the vicinity of 30x30.  An increase in the number of epochs either reduced accuracy or had no effect other than to allow randomness to drop the result by a few fractions of a percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN conclusions:\n",
    "Almost as time-intensive as CNN, but with less accuracy to show for it.\n",
    "\n",
    "Though increasing the number of epochs did improve performance, accuracy gains leveled off around the 90th percentile, and then proceeded to fall.\n",
    "\n",
    "Dropping the image scale to 20x20 improved performance, as expected, though I'm uncertain how much of the sudden spike in val_acc in epoch 10 of v3 was due to a lucky random fluke or actual improvement in performance.  Epoch 9 saw a *drop* in val_acc, to .88, which then bounced up to .94 for epoch 10.  On the other hand, the accuracy (acc, not val_acc) of the model had steadily increased throughout.  The combination of these two implies to me that the model *was* becoming more accurate, and the last epoch simply saw a marked reduction in overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from IPython.display import display\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "import pydotplus\n",
    "from itertools import cycle\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras and Tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# Import various componenets for model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD, adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K\n",
    "\n",
    "# import PIL\n",
    "import PIL as pil\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KERAS\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "from PIL import Image\n",
    "from numpy import *\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tinker_Test', '.DS_Store', 'LICENSE', 'Test', 'papers', 'Tinker_Train', 'Training', 'readme.md', 'test-multiple_fruits']\n"
     ]
    }
   ],
   "source": [
    "# verify that we have the folder in the correct place, and what the subfolders are labeled\n",
    "print(os.listdir(\"fruits-360\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that, given a folder name, will return:\n",
    "    # 1) the datapath for that folder and \n",
    "    # 2) a list of the folders inside\n",
    "\n",
    "def get_datapath(folder):\n",
    "    data_path = 'fruits-360/' + str(folder) + '/'\n",
    "    fruits_list = os.listdir(data_path) \n",
    "    if '.DS_Store' in fruits_list:\n",
    "        fruits_list.remove('.DS_Store')\n",
    "    return(data_path, fruits_list)\n",
    "\n",
    "\n",
    "# function that, given a folder and a number (X) of fruit-folders to pick, returns: \n",
    "    # 1) a list of the image pathways in the selected folders (folder_image_list)\n",
    "    # 2) a list of the fruit labels for each of those images (image_label_list)\n",
    "\n",
    "def get_image_list(folder,num_fruits):\n",
    "    data_path, fruits_list = get_datapath(folder)\n",
    "    \n",
    "    folder_image_list = []\n",
    "    image_label_list = []\n",
    "    image_numlabel_list = []\n",
    "    fruit_counter = 0\n",
    "    \n",
    "    fruit = fruits_list[0]\n",
    "    data_path_fruit = str(data_path + str(fruit) + '/')\n",
    "    image_list_fruit = os.listdir(data_path_fruit) \n",
    "\n",
    "    for fruit in fruits_list[:num_fruits]:\n",
    "        data_path_fruit = str(data_path + str(fruit) + '/')\n",
    "        image_list_fruit = os.listdir(data_path_fruit)\n",
    "        \n",
    "        for image in image_list_fruit:\n",
    "            image_address = str(data_path_fruit + str(image))\n",
    "            folder_image_list.append(image_address)\n",
    "            image_label_list.append(fruit)\n",
    "            image_numlabel_list.append(fruit_counter)\n",
    "\n",
    "        fruit_counter += 1\n",
    "\n",
    "    return(folder_image_list, image_label_list, image_numlabel_list)\n",
    "\n",
    "# could also use this format datapath.split(\"/\")[-1] to grab the fruit label of a image\n",
    "\n",
    "\n",
    "# function that, given a folder and a number (X) of fruit-folders to pick, returns: \n",
    "    # 1) a list of the image pathways in the selected folders (folder_image_list)\n",
    "    # 2) a list of the fruit labels for each of those images (folder_label_list)\n",
    "    # 3) a list of arrays representing each image in the list of image pathways (image_array_list)\n",
    "\n",
    "def get_image_arrays(folder, num_fruits, image_scale):\n",
    "    print('Working on ' + folder + ' images...', end = '')\n",
    "\n",
    "    folder_image_list, folder_label_list, folder_numlabel_list = get_image_list(folder,num_fruits)\n",
    "    image_array_list = []\n",
    "    \n",
    "    for image in folder_image_list:\n",
    "        image_jpg = load_img(image, target_size=(image_scale, image_scale))\n",
    "        image_array = img_to_array(image_jpg)\n",
    "        image_array /= 255\n",
    "        image_array_list.append(image_array)\n",
    "\n",
    "    print('  Done with upload and initial processing.')\n",
    "    return(folder_image_list, folder_label_list, folder_numlabel_list, image_array_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 15\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])  # takes a snapshot of a few images before flattening\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n"
     ]
    }
   ],
   "source": [
    "# boil down the size of the input arrays for the model\n",
    "input_shape = str(train_images[1].shape)\n",
    "input_shape = input_shape.replace('(','')\n",
    "input_shape = input_shape.replace(',)','')\n",
    "input_shape = int(input_shape)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 64)                43264     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 100)               6500      \n",
      "=================================================================\n",
      "Total params: 53,924\n",
      "Trainable params: 53,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(input_shape,)))  \n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/10\n",
      "51770/51770 [==============================] - 2s 47us/step - loss: 2.6653 - acc: 0.3020 - val_loss: 1.1949 - val_acc: 0.7096\n",
      "Epoch 2/10\n",
      "51770/51770 [==============================] - 2s 34us/step - loss: 1.1461 - acc: 0.6371 - val_loss: 0.7102 - val_acc: 0.8025\n",
      "Epoch 3/10\n",
      "51770/51770 [==============================] - 2s 31us/step - loss: 0.7682 - acc: 0.7465 - val_loss: 0.5782 - val_acc: 0.8278\n",
      "Epoch 4/10\n",
      "51770/51770 [==============================] - 2s 32us/step - loss: 0.5849 - acc: 0.8063 - val_loss: 0.4276 - val_acc: 0.8681\n",
      "Epoch 5/10\n",
      "51770/51770 [==============================] - 2s 31us/step - loss: 0.4712 - acc: 0.8438 - val_loss: 0.3998 - val_acc: 0.8771\n",
      "Epoch 6/10\n",
      "51770/51770 [==============================] - 2s 31us/step - loss: 0.3966 - acc: 0.8671 - val_loss: 0.4275 - val_acc: 0.8811\n",
      "Epoch 7/10\n",
      "51770/51770 [==============================] - 2s 30us/step - loss: 0.3393 - acc: 0.8860 - val_loss: 0.3272 - val_acc: 0.9045\n",
      "Epoch 8/10\n",
      "51770/51770 [==============================] - 2s 30us/step - loss: 0.3001 - acc: 0.8989 - val_loss: 0.4126 - val_acc: 0.8700\n",
      "Epoch 9/10\n",
      "51770/51770 [==============================] - 2s 31us/step - loss: 0.2707 - acc: 0.9096 - val_loss: 0.3932 - val_acc: 0.8818\n",
      "Epoch 10/10\n",
      "51770/51770 [==============================] - 2s 30us/step - loss: 0.2464 - acc: 0.9175 - val_loss: 0.4012 - val_acc: 0.8869\n",
      "Test loss: 0.401205202729281\n",
      "Test accuracy: 0.8869129834254144\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential MLP v2, 15 Epochs, scale = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 15\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])  # takes a snapshot of a few images before flattening\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675\n"
     ]
    }
   ],
   "source": [
    "# boil down the size of the input arrays for the model\n",
    "input_shape = str(train_images[1].shape)\n",
    "input_shape = input_shape.replace('(','')\n",
    "input_shape = input_shape.replace(',)','')\n",
    "input_shape = int(input_shape)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 64)                43264     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               6500      \n",
      "=================================================================\n",
      "Total params: 53,924\n",
      "Trainable params: 53,924\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(input_shape,)))  \n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/15\n",
      "51770/51770 [==============================] - 2s 37us/step - loss: 2.6241 - acc: 0.3155 - val_loss: 1.2014 - val_acc: 0.6907\n",
      "Epoch 2/15\n",
      "51770/51770 [==============================] - 1s 29us/step - loss: 1.1429 - acc: 0.6389 - val_loss: 0.8778 - val_acc: 0.7520\n",
      "Epoch 3/15\n",
      "51770/51770 [==============================] - 2s 29us/step - loss: 0.7811 - acc: 0.7460 - val_loss: 0.5835 - val_acc: 0.8291\n",
      "Epoch 4/15\n",
      "51770/51770 [==============================] - 2s 32us/step - loss: 0.6071 - acc: 0.8013 - val_loss: 0.4547 - val_acc: 0.8578\n",
      "Epoch 5/15\n",
      "51770/51770 [==============================] - 2s 35us/step - loss: 0.4894 - acc: 0.8364 - val_loss: 0.3933 - val_acc: 0.8820\n",
      "Epoch 6/15\n",
      "51770/51770 [==============================] - 2s 42us/step - loss: 0.4158 - acc: 0.8604 - val_loss: 0.4007 - val_acc: 0.8733\n",
      "Epoch 7/15\n",
      "51770/51770 [==============================] - 2s 31us/step - loss: 0.3520 - acc: 0.8835 - val_loss: 0.4649 - val_acc: 0.8493\n",
      "Epoch 8/15\n",
      "51770/51770 [==============================] - 2s 29us/step - loss: 0.3134 - acc: 0.8948 - val_loss: 0.3531 - val_acc: 0.8822\n",
      "Epoch 9/15\n",
      "51770/51770 [==============================] - 1s 29us/step - loss: 0.2789 - acc: 0.9048 - val_loss: 0.3715 - val_acc: 0.8850\n",
      "Epoch 10/15\n",
      "51770/51770 [==============================] - 2s 30us/step - loss: 0.2512 - acc: 0.9148 - val_loss: 0.3198 - val_acc: 0.8887\n",
      "Epoch 11/15\n",
      "51770/51770 [==============================] - 2s 30us/step - loss: 0.2292 - acc: 0.9230 - val_loss: 0.4047 - val_acc: 0.8719\n",
      "Epoch 12/15\n",
      "51770/51770 [==============================] - 2s 40us/step - loss: 0.2150 - acc: 0.9273 - val_loss: 0.3322 - val_acc: 0.8906\n",
      "Epoch 13/15\n",
      "51770/51770 [==============================] - 2s 40us/step - loss: 0.1988 - acc: 0.9325 - val_loss: 0.2545 - val_acc: 0.9220\n",
      "Epoch 14/15\n",
      "51770/51770 [==============================] - 2s 37us/step - loss: 0.1880 - acc: 0.9369 - val_loss: 0.3137 - val_acc: 0.9048\n",
      "Epoch 15/15\n",
      "51770/51770 [==============================] - 2s 31us/step - loss: 0.1727 - acc: 0.9396 - val_loss: 0.3399 - val_acc: 0.8953\n",
      "Test loss: 0.33993036568416374\n",
      "Test accuracy: 0.8952578268876611\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential MLP v3, 15 Epochs, scale = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 25\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])  # takes a snapshot of a few images before flattening\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "# boil down the size of the input arrays for the model\n",
    "input_shape = str(train_images[1].shape)\n",
    "input_shape = input_shape.replace('(','')\n",
    "input_shape = input_shape.replace(',)','')\n",
    "input_shape = int(input_shape)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 64)                120064    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               6500      \n",
      "=================================================================\n",
      "Total params: 130,724\n",
      "Trainable params: 130,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(input_shape,)))  \n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/15\n",
      "51770/51770 [==============================] - 3s 62us/step - loss: 2.9514 - acc: 0.2444 - val_loss: 1.3333 - val_acc: 0.6796\n",
      "Epoch 2/15\n",
      "51770/51770 [==============================] - 3s 52us/step - loss: 1.4741 - acc: 0.5440 - val_loss: 0.8309 - val_acc: 0.7716\n",
      "Epoch 3/15\n",
      "51770/51770 [==============================] - 3s 55us/step - loss: 1.1121 - acc: 0.6479 - val_loss: 0.7837 - val_acc: 0.7604\n",
      "Epoch 4/15\n",
      "51770/51770 [==============================] - 3s 61us/step - loss: 0.9216 - acc: 0.7055 - val_loss: 0.6781 - val_acc: 0.8043\n",
      "Epoch 5/15\n",
      "51770/51770 [==============================] - 3s 53us/step - loss: 0.7964 - acc: 0.7432 - val_loss: 0.6014 - val_acc: 0.8242\n",
      "Epoch 6/15\n",
      "51770/51770 [==============================] - 3s 61us/step - loss: 0.7090 - acc: 0.7736 - val_loss: 0.4764 - val_acc: 0.8597\n",
      "Epoch 7/15\n",
      "51770/51770 [==============================] - 3s 53us/step - loss: 0.6519 - acc: 0.7914 - val_loss: 0.4863 - val_acc: 0.8562\n",
      "Epoch 8/15\n",
      "51770/51770 [==============================] - 3s 55us/step - loss: 0.6050 - acc: 0.8059 - val_loss: 0.4472 - val_acc: 0.8621\n",
      "Epoch 9/15\n",
      "51770/51770 [==============================] - 3s 54us/step - loss: 0.5631 - acc: 0.8194 - val_loss: 0.5204 - val_acc: 0.8388\n",
      "Epoch 10/15\n",
      "51770/51770 [==============================] - 3s 61us/step - loss: 0.5261 - acc: 0.8316 - val_loss: 0.4113 - val_acc: 0.8706\n",
      "Epoch 11/15\n",
      "51770/51770 [==============================] - 3s 59us/step - loss: 0.5015 - acc: 0.8413 - val_loss: 0.4239 - val_acc: 0.8692\n",
      "Epoch 12/15\n",
      "51770/51770 [==============================] - 3s 54us/step - loss: 0.4740 - acc: 0.8501 - val_loss: 0.4068 - val_acc: 0.8850\n",
      "Epoch 13/15\n",
      "51770/51770 [==============================] - 3s 60us/step - loss: 0.4533 - acc: 0.8559 - val_loss: 0.4224 - val_acc: 0.8726\n",
      "Epoch 14/15\n",
      "51770/51770 [==============================] - 3s 54us/step - loss: 0.4288 - acc: 0.8633 - val_loss: 0.3799 - val_acc: 0.8818\n",
      "Epoch 15/15\n",
      "51770/51770 [==============================] - 3s 56us/step - loss: 0.4141 - acc: 0.8686 - val_loss: 0.4082 - val_acc: 0.8637\n",
      "Test loss: 0.408243445859689\n",
      "Test accuracy: 0.8636625230202578\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq MLP Conclusion\n",
    "Decent, and very fast (once you've uploaded the images) but not in the 90%+ category yet. \n",
    "\n",
    "Extra Epochs seems to do little, as by #10 it seems to have reached a point of minimal or negative returns, where the random associations in each iteration cause it to \"forget\" how to label some fruits even as it learns how to label others.\n",
    "\n",
    "The most critical factor seems to be the image size, as a scale of 100 pixels (the scale of the images in the download for this collection) produces abysmal results, and gets progressively better until it reaches somewhere between 10 and 20, after which further decreases drop performance (likely due to simply not having enough identifying data to go on in each image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network, v1 (image_scale = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 50\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n",
      "Test images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Test images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n",
      "Train images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Train images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/10\n",
      "51770/51770 [==============================] - 436s 8ms/step - loss: 1.8939 - acc: 0.5137 - val_loss: 0.3347 - val_acc: 0.9075\n",
      "Epoch 2/10\n",
      "51770/51770 [==============================] - 423s 8ms/step - loss: 0.3434 - acc: 0.8890 - val_loss: 0.1378 - val_acc: 0.9587\n",
      "Epoch 3/10\n",
      "51770/51770 [==============================] - 422s 8ms/step - loss: 0.1745 - acc: 0.9412 - val_loss: 0.1283 - val_acc: 0.9649\n",
      "Epoch 4/10\n",
      "51770/51770 [==============================] - 411s 8ms/step - loss: 0.1175 - acc: 0.9607 - val_loss: 0.0925 - val_acc: 0.9733\n",
      "Epoch 5/10\n",
      "51770/51770 [==============================] - 406s 8ms/step - loss: 0.0847 - acc: 0.9710 - val_loss: 0.1040 - val_acc: 0.9786\n",
      "Epoch 6/10\n",
      "51770/51770 [==============================] - 417s 8ms/step - loss: 0.0680 - acc: 0.9766 - val_loss: 0.0958 - val_acc: 0.9796\n",
      "Epoch 7/10\n",
      "51770/51770 [==============================] - 423s 8ms/step - loss: 0.0533 - acc: 0.9821 - val_loss: 0.0869 - val_acc: 0.9779\n",
      "Epoch 8/10\n",
      "51770/51770 [==============================] - 425s 8ms/step - loss: 0.0469 - acc: 0.9841 - val_loss: 0.0759 - val_acc: 0.9814\n",
      "Epoch 9/10\n",
      "51770/51770 [==============================] - 413s 8ms/step - loss: 0.0434 - acc: 0.9847 - val_loss: 0.0750 - val_acc: 0.9845\n",
      "Epoch 10/10\n",
      "51770/51770 [==============================] - 405s 8ms/step - loss: 0.0373 - acc: 0.9872 - val_loss: 0.0827 - val_acc: 0.9849\n",
      "Test loss: 0.08268193309910271\n",
      "Test accuracy: 0.9849217311233885\n"
     ]
    }
   ],
   "source": [
    "input_shape = (image_scale, image_scale, 3)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network, v2 (image_scale = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 30\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n",
      "Test images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Test images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n",
      "Train images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Train images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/10\n",
      "51770/51770 [==============================] - 139s 3ms/step - loss: 1.7889 - acc: 0.5337 - val_loss: 0.2749 - val_acc: 0.9141\n",
      "Epoch 2/10\n",
      "51770/51770 [==============================] - 138s 3ms/step - loss: 0.2570 - acc: 0.9177 - val_loss: 0.1699 - val_acc: 0.9461\n",
      "Epoch 3/10\n",
      "51770/51770 [==============================] - 137s 3ms/step - loss: 0.1233 - acc: 0.9595 - val_loss: 0.1078 - val_acc: 0.9639\n",
      "Epoch 4/10\n",
      "51770/51770 [==============================] - 136s 3ms/step - loss: 0.0813 - acc: 0.9729 - val_loss: 0.1152 - val_acc: 0.9611\n",
      "Epoch 5/10\n",
      "51770/51770 [==============================] - 136s 3ms/step - loss: 0.0611 - acc: 0.9798 - val_loss: 0.0749 - val_acc: 0.9776\n",
      "Epoch 6/10\n",
      "51770/51770 [==============================] - 137s 3ms/step - loss: 0.0481 - acc: 0.9836 - val_loss: 0.0714 - val_acc: 0.9784\n",
      "Epoch 7/10\n",
      "51770/51770 [==============================] - 137s 3ms/step - loss: 0.0392 - acc: 0.9868 - val_loss: 0.0988 - val_acc: 0.9749\n",
      "Epoch 8/10\n",
      "51770/51770 [==============================] - 137s 3ms/step - loss: 0.0337 - acc: 0.9888 - val_loss: 0.0805 - val_acc: 0.9770\n",
      "Epoch 9/10\n",
      "51770/51770 [==============================] - 135s 3ms/step - loss: 0.0292 - acc: 0.9905 - val_loss: 0.0971 - val_acc: 0.9767\n",
      "Epoch 10/10\n",
      "51770/51770 [==============================] - 134s 3ms/step - loss: 0.0302 - acc: 0.9901 - val_loss: 0.0624 - val_acc: 0.9858\n",
      "Test loss: 0.06239043491176314\n",
      "Test accuracy: 0.9858425414364641\n"
     ]
    }
   ],
   "source": [
    "input_shape = (image_scale, image_scale, 3)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network, v3 (image_scale = 30, kernels = 4x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 30\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n",
      "Test images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Test images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n",
      "Train images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Train images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/10\n",
      "51770/51770 [==============================] - 193s 4ms/step - loss: 1.7834 - acc: 0.5497 - val_loss: 0.2610 - val_acc: 0.9243\n",
      "Epoch 2/10\n",
      "51770/51770 [==============================] - 193s 4ms/step - loss: 0.2243 - acc: 0.9289 - val_loss: 0.1503 - val_acc: 0.9517\n",
      "Epoch 3/10\n",
      "51770/51770 [==============================] - 200s 4ms/step - loss: 0.1078 - acc: 0.9639 - val_loss: 0.1262 - val_acc: 0.9639\n",
      "Epoch 4/10\n",
      "51770/51770 [==============================] - 188s 4ms/step - loss: 0.0732 - acc: 0.9761 - val_loss: 0.1187 - val_acc: 0.9655\n",
      "Epoch 5/10\n",
      "51770/51770 [==============================] - 191s 4ms/step - loss: 0.0519 - acc: 0.9828 - val_loss: 0.0953 - val_acc: 0.9746\n",
      "Epoch 6/10\n",
      "51770/51770 [==============================] - 191s 4ms/step - loss: 0.0411 - acc: 0.9863 - val_loss: 0.0978 - val_acc: 0.9732\n",
      "Epoch 7/10\n",
      "51770/51770 [==============================] - 187s 4ms/step - loss: 0.0343 - acc: 0.9887 - val_loss: 0.0960 - val_acc: 0.9717\n",
      "Epoch 8/10\n",
      "51770/51770 [==============================] - 194s 4ms/step - loss: 0.0295 - acc: 0.9906 - val_loss: 0.0732 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "51770/51770 [==============================] - 193s 4ms/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.0731 - val_acc: 0.9830\n",
      "Epoch 10/10\n",
      "51770/51770 [==============================] - 203s 4ms/step - loss: 0.0251 - acc: 0.9918 - val_loss: 0.1036 - val_acc: 0.9785\n",
      "Test loss: 0.10361308509567997\n",
      "Test accuracy: 0.9784760589318601\n"
     ]
    }
   ],
   "source": [
    "input_shape = (image_scale, image_scale, 3)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(4, 4),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,))\n",
    "model.add(Conv2D(64, (4, 4), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network, v4 (image_scale = 30, kernels = 3x3, epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 30\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n",
      "Test images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Test images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n",
      "Train images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Train images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/15\n",
      "51770/51770 [==============================] - 155s 3ms/step - loss: 1.8415 - acc: 0.5223 - val_loss: 0.3420 - val_acc: 0.8959\n",
      "Epoch 2/15\n",
      "51770/51770 [==============================] - 186s 4ms/step - loss: 0.2792 - acc: 0.9084 - val_loss: 0.1269 - val_acc: 0.9625\n",
      "Epoch 3/15\n",
      "51770/51770 [==============================] - 191s 4ms/step - loss: 0.1368 - acc: 0.9545 - val_loss: 0.0990 - val_acc: 0.9694\n",
      "Epoch 4/15\n",
      "51770/51770 [==============================] - 196s 4ms/step - loss: 0.0895 - acc: 0.9706 - val_loss: 0.0735 - val_acc: 0.9791\n",
      "Epoch 5/15\n",
      "51770/51770 [==============================] - 163s 3ms/step - loss: 0.0677 - acc: 0.9776 - val_loss: 0.0832 - val_acc: 0.9765\n",
      "Epoch 6/15\n",
      "51770/51770 [==============================] - 156s 3ms/step - loss: 0.0523 - acc: 0.9826 - val_loss: 0.0727 - val_acc: 0.9808\n",
      "Epoch 7/15\n",
      "51770/51770 [==============================] - 163s 3ms/step - loss: 0.0441 - acc: 0.9854 - val_loss: 0.0560 - val_acc: 0.9861\n",
      "Epoch 8/15\n",
      "51770/51770 [==============================] - 198s 4ms/step - loss: 0.0402 - acc: 0.9865 - val_loss: 0.0550 - val_acc: 0.9837\n",
      "Epoch 9/15\n",
      "51770/51770 [==============================] - 192s 4ms/step - loss: 0.0351 - acc: 0.9882 - val_loss: 0.0454 - val_acc: 0.9856\n",
      "Epoch 10/15\n",
      "51770/51770 [==============================] - 191s 4ms/step - loss: 0.0319 - acc: 0.9890 - val_loss: 0.0533 - val_acc: 0.9845\n",
      "Epoch 11/15\n",
      "51770/51770 [==============================] - 155s 3ms/step - loss: 0.0269 - acc: 0.9909 - val_loss: 0.0493 - val_acc: 0.9873\n",
      "Epoch 12/15\n",
      "51770/51770 [==============================] - 144s 3ms/step - loss: 0.0279 - acc: 0.9904 - val_loss: 0.0538 - val_acc: 0.9871\n",
      "Epoch 13/15\n",
      "51770/51770 [==============================] - 145s 3ms/step - loss: 0.0238 - acc: 0.9915 - val_loss: 0.0789 - val_acc: 0.9835\n",
      "Epoch 14/15\n",
      "51770/51770 [==============================] - 144s 3ms/step - loss: 0.0228 - acc: 0.9924 - val_loss: 0.0613 - val_acc: 0.9834\n",
      "Epoch 15/15\n",
      "51770/51770 [==============================] - 151s 3ms/step - loss: 0.0210 - acc: 0.9929 - val_loss: 0.0691 - val_acc: 0.9833\n",
      "Test loss: 0.06906120427659183\n",
      "Test accuracy: 0.9832527624309392\n"
     ]
    }
   ],
   "source": [
    "input_shape = (image_scale, image_scale, 3)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=15,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network, v5 (image_scale = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 20\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n",
      "Test images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Test images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n",
      "Train images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Train images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/10\n",
      "51770/51770 [==============================] - 71s 1ms/step - loss: 1.8746 - acc: 0.5093 - val_loss: 0.3800 - val_acc: 0.8915\n",
      "Epoch 2/10\n",
      "51770/51770 [==============================] - 68s 1ms/step - loss: 0.3172 - acc: 0.8980 - val_loss: 0.1663 - val_acc: 0.9432\n",
      "Epoch 3/10\n",
      "51770/51770 [==============================] - 74s 1ms/step - loss: 0.1568 - acc: 0.9491 - val_loss: 0.1471 - val_acc: 0.9560\n",
      "Epoch 4/10\n",
      "51770/51770 [==============================] - 64s 1ms/step - loss: 0.1022 - acc: 0.9659 - val_loss: 0.0962 - val_acc: 0.9743\n",
      "Epoch 5/10\n",
      "51770/51770 [==============================] - 63s 1ms/step - loss: 0.0780 - acc: 0.9738 - val_loss: 0.0935 - val_acc: 0.9723\n",
      "Epoch 6/10\n",
      "51770/51770 [==============================] - 64s 1ms/step - loss: 0.0612 - acc: 0.9794 - val_loss: 0.0797 - val_acc: 0.9770\n",
      "Epoch 7/10\n",
      "51770/51770 [==============================] - 63s 1ms/step - loss: 0.0504 - acc: 0.9831 - val_loss: 0.0889 - val_acc: 0.9784\n",
      "Epoch 8/10\n",
      "51770/51770 [==============================] - 64s 1ms/step - loss: 0.0425 - acc: 0.9860 - val_loss: 0.0891 - val_acc: 0.9733\n",
      "Epoch 9/10\n",
      "51770/51770 [==============================] - 64s 1ms/step - loss: 0.0379 - acc: 0.9873 - val_loss: 0.0665 - val_acc: 0.9822\n",
      "Epoch 10/10\n",
      "51770/51770 [==============================] - 64s 1ms/step - loss: 0.0335 - acc: 0.9893 - val_loss: 0.0724 - val_acc: 0.9812\n",
      "Test loss: 0.07242357408790706\n",
      "Test accuracy: 0.9812384898710865\n"
     ]
    }
   ],
   "source": [
    "input_shape = (image_scale, image_scale, 3)\n",
    "\n",
    "\n",
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(number_fruits, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "score = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN conclusions:\n",
    "The most effective of the three methods, though also the most time-intensive.\n",
    "\n",
    "Increasing the kernel size seemed to reduce performance.  Optimal image size was somewhere in the vicinity of 30x30.  An increase in the number of epochs either reduced accuracy or had no effect other than to allow randomness to drop the result by a few fractions of a percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network, v1 (image_scale = 30, Epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 30\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n",
      "Test images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Test images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n",
      "Train images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Train images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (51770, 30, 30, 3)\n",
      "51770 train samples\n",
      "17376 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters.\n",
    "batch_size = 64\n",
    "# num_classes = number_fruits\n",
    "epochs = 3\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "print('x_train shape:', train_images.shape)\n",
    "print(train_images.shape[0], 'train samples')\n",
    "print(test_images.shape[0], 'test samples')\n",
    "\n",
    "row, col, pixel = train_images.shape[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/3\n",
      "51770/51770 [==============================] - 152s 3ms/step - loss: 3.5909 - acc: 0.1292 - val_loss: 2.7050 - val_acc: 0.3187\n",
      "Epoch 2/3\n",
      "51770/51770 [==============================] - 148s 3ms/step - loss: 2.1660 - acc: 0.4402 - val_loss: 1.8487 - val_acc: 0.4695\n",
      "Epoch 3/3\n",
      "51770/51770 [==============================] - 146s 3ms/step - loss: 1.3842 - acc: 0.6448 - val_loss: 1.2616 - val_acc: 0.6901\n",
      "Test loss: 1.261625601853455\n",
      "Test accuracy: 0.6900897790055248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(number_fruits, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network v2, Epochs = 15, Scale = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (51770, 30, 30, 3)\n",
      "51770 train samples\n",
      "17376 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters.\n",
    "batch_size = 64\n",
    "# num_classes = number_fruits\n",
    "epochs = 15\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "print('x_train shape:', train_images.shape)\n",
    "print(train_images.shape[0], 'train samples')\n",
    "print(test_images.shape[0], 'test samples')\n",
    "\n",
    "row, col, pixel = train_images.shape[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/15\n",
      "51770/51770 [==============================] - 154s 3ms/step - loss: 3.5835 - acc: 0.1400 - val_loss: 2.7246 - val_acc: 0.3231\n",
      "Epoch 2/15\n",
      "51770/51770 [==============================] - 150s 3ms/step - loss: 2.1960 - acc: 0.4303 - val_loss: 1.8402 - val_acc: 0.5147\n",
      "Epoch 3/15\n",
      "51770/51770 [==============================] - 154s 3ms/step - loss: 1.4594 - acc: 0.6189 - val_loss: 1.4940 - val_acc: 0.5858\n",
      "Epoch 4/15\n",
      "51770/51770 [==============================] - 150s 3ms/step - loss: 0.9828 - acc: 0.7488 - val_loss: 0.9887 - val_acc: 0.7278\n",
      "Epoch 5/15\n",
      "51770/51770 [==============================] - 167s 3ms/step - loss: 0.6737 - acc: 0.8279 - val_loss: 0.7790 - val_acc: 0.7967\n",
      "Epoch 6/15\n",
      "51770/51770 [==============================] - 157s 3ms/step - loss: 0.4690 - acc: 0.8786 - val_loss: 0.6289 - val_acc: 0.8231\n",
      "Epoch 7/15\n",
      "51770/51770 [==============================] - 160s 3ms/step - loss: 0.3462 - acc: 0.9087 - val_loss: 0.4708 - val_acc: 0.8707\n",
      "Epoch 8/15\n",
      "51770/51770 [==============================] - 158s 3ms/step - loss: 0.2628 - acc: 0.9325 - val_loss: 0.5053 - val_acc: 0.8566\n",
      "Epoch 9/15\n",
      "51770/51770 [==============================] - 155s 3ms/step - loss: 0.2085 - acc: 0.9472 - val_loss: 0.4997 - val_acc: 0.8477\n",
      "Epoch 10/15\n",
      "51770/51770 [==============================] - 182s 4ms/step - loss: 0.1649 - acc: 0.9587 - val_loss: 0.4019 - val_acc: 0.8907\n",
      "Epoch 11/15\n",
      "51770/51770 [==============================] - 151s 3ms/step - loss: 0.1314 - acc: 0.9691 - val_loss: 0.2869 - val_acc: 0.9204\n",
      "Epoch 12/15\n",
      "51770/51770 [==============================] - 151s 3ms/step - loss: 0.1068 - acc: 0.9745 - val_loss: 0.3469 - val_acc: 0.8983\n",
      "Epoch 13/15\n",
      "51770/51770 [==============================] - 172s 3ms/step - loss: 0.0893 - acc: 0.9786 - val_loss: 0.6137 - val_acc: 0.8468\n",
      "Epoch 14/15\n",
      "51770/51770 [==============================] - 151s 3ms/step - loss: 0.0771 - acc: 0.9818 - val_loss: 0.3551 - val_acc: 0.9058\n",
      "Epoch 15/15\n",
      "51770/51770 [==============================] - 166s 3ms/step - loss: 0.0631 - acc: 0.9847 - val_loss: 0.5071 - val_acc: 0.8462\n",
      "Test loss: 0.5071374688735515\n",
      "Test accuracy: 0.8462246777163904\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(number_fruits, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network v3, Epochs = 10, scale = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix our settings\n",
    "\n",
    "number_fruits = 100\n",
    "image_scale = 20\n",
    "\n",
    "sample_list = [.01,.02,.03,.04,.05, .1,.2,.3,.4,.5,.6,.7,.8,.9,.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Test images...  Done with upload and initial processing.\n",
      "Test images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the test dataset\n",
    "\n",
    "test_datapaths, test_labels, test_numlabels, test_images = get_image_arrays('Test', \n",
    "                                                                            number_fruits, \n",
    "                                                                            image_scale)\n",
    "test_sample = []\n",
    "for x in sample_list:\n",
    "    test_sample.append(test_images[int(x*len(test_images))])\n",
    "\n",
    "for x in range(0,len(test_images)):\n",
    "    test_images[x] = np.ndarray.flatten(test_images[x])\n",
    "\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "y_test = keras.utils.to_categorical(test_numlabels, number_fruits)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Test images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Training images...  Done with upload and initial processing.\n",
      "Train images processing is complete\n"
     ]
    }
   ],
   "source": [
    "# Work through the train dataset\n",
    "\n",
    "train_datapaths, train_labels, train_numlabels, train_images = get_image_arrays('Training', \n",
    "                                                                                number_fruits, \n",
    "                                                                                image_scale)\n",
    "train_sample = []\n",
    "for x in sample_list:\n",
    "    train_sample.append(train_images[int(x*len(train_images))])\n",
    "\n",
    "for x in range(0,len(train_images)):\n",
    "    train_images[x] = np.ndarray.flatten(train_images[x])\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "\n",
    "y_train = keras.utils.to_categorical(train_numlabels, number_fruits)\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], image_scale, image_scale, 3)\n",
    "\n",
    "print('Train images processing is complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (51770, 20, 20, 3)\n",
      "51770 train samples\n",
      "17376 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training parameters.\n",
    "batch_size = 64\n",
    "# num_classes = number_fruits\n",
    "epochs = 10\n",
    "\n",
    "# Embedding dimensions.\n",
    "row_hidden = 32\n",
    "col_hidden = 32\n",
    "\n",
    "# Reshapes data to 4D for Hierarchical RNN.\n",
    "print('x_train shape:', train_images.shape)\n",
    "print(train_images.shape[0], 'train samples')\n",
    "print(test_images.shape[0], 'test samples')\n",
    "\n",
    "row, col, pixel = train_images.shape[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51770 samples, validate on 17376 samples\n",
      "Epoch 1/10\n",
      "51770/51770 [==============================] - 83s 2ms/step - loss: 3.4980 - acc: 0.1656 - val_loss: 2.3979 - val_acc: 0.4134\n",
      "Epoch 2/10\n",
      "51770/51770 [==============================] - 93s 2ms/step - loss: 1.8562 - acc: 0.5359 - val_loss: 1.6999 - val_acc: 0.5165\n",
      "Epoch 3/10\n",
      "51770/51770 [==============================] - 81s 2ms/step - loss: 1.1135 - acc: 0.7228 - val_loss: 1.0869 - val_acc: 0.7203\n",
      "Epoch 4/10\n",
      "51770/51770 [==============================] - 74s 1ms/step - loss: 0.7040 - acc: 0.8239 - val_loss: 0.7144 - val_acc: 0.8147\n",
      "Epoch 5/10\n",
      "51770/51770 [==============================] - 76s 1ms/step - loss: 0.4747 - acc: 0.8832 - val_loss: 1.0376 - val_acc: 0.6896\n",
      "Epoch 6/10\n",
      "51770/51770 [==============================] - 71s 1ms/step - loss: 0.3354 - acc: 0.9169 - val_loss: 0.5667 - val_acc: 0.8371\n",
      "Epoch 7/10\n",
      "51770/51770 [==============================] - 71s 1ms/step - loss: 0.2451 - acc: 0.9406 - val_loss: 0.4391 - val_acc: 0.8804\n",
      "Epoch 8/10\n",
      "51770/51770 [==============================] - 71s 1ms/step - loss: 0.1809 - acc: 0.9572 - val_loss: 0.3191 - val_acc: 0.9146\n",
      "Epoch 9/10\n",
      "51770/51770 [==============================] - 72s 1ms/step - loss: 0.1387 - acc: 0.9684 - val_loss: 0.3922 - val_acc: 0.8871\n",
      "Epoch 10/10\n",
      "51770/51770 [==============================] - 73s 1ms/step - loss: 0.1117 - acc: 0.9740 - val_loss: 0.2210 - val_acc: 0.9424\n",
      "Test loss: 0.22095837174950367\n",
      "Test accuracy: 0.9423918047882136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4D input.\n",
    "x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# Encodes columns of encoded rows.\n",
    "encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "prediction = Dense(number_fruits, activation='softmax')(encoded_columns)\n",
    "model = Model(x, prediction)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "model.fit(train_images, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_images, y_test))\n",
    "\n",
    "# Evaluation.\n",
    "scores = model.evaluate(test_images, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "os.system('say \"all done.\"'); print('\\a')  # this could take a while, let me know when it's done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN conclusions:\n",
    "Almost as time-intensive as CNN, but with less accuracy to show for it.\n",
    "\n",
    "Though increasing the number of epochs did improve performance, accuracy gains leveled off around the 90th percentile, and then proceeded to fall.\n",
    "\n",
    "Dropping the image scale to 20x20 improved performance, as expected, though I'm uncertain how much of the sudden spike in val_acc in epoch 10 of v3 was due to a lucky random fluke or actual improvement in performance.  Epoch 9 saw a *drop* in val_acc, to .88, which then bounced up to .94 for epoch 10.  On the other hand, the accuracy (acc, not val_acc) of the model had steadily increased throughout.  The combination of these two implies to me that the model *was* becoming more accurate, and the last epoch simply saw a marked reduction in overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
